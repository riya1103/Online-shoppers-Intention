{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > # Online shoppers intention classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from pylab import rcParams\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (<ipython-input-4-2480efce1ccc>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-2480efce1ccc>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    df = pd.read_csv(\"C:\\Users\\dell\\Desktop\\online_shoppers_intention.csv\")\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\Users\\dell\\Desktop\\online_shoppers_intention.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # EDA and preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(df.applymap(lambda x: x == 'nan'))\n",
    "np.where(pd.isnull(df))\n",
    "df.loc[1065][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Administrative'].mode()\n",
    "df['Administrative_Duration'].mode()\n",
    "df['Informational'].mode()\n",
    "df['Informational_Duration'].mode()\n",
    "df.mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* > Using np.nan to replace those null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Month'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weekend'] = df['Weekend'].replace(False,0)\n",
    "df['Weekend'] = df['Weekend'].replace(True,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weekend'] = df['Weekend'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Revenue'] = df['Revenue'].replace(False,0)\n",
    "df['Revenue'] = df['Revenue'].replace(True,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Revenue'] = df['Revenue'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(columns=['Month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Revenue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='Revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VisitorType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Revenue'].value_counts()\n",
    "1908/(1908+10422)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# from sklearn.preprocessing import Normalizer\n",
    "X = preprocessing.normalize(X, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # GENERATE MODEL REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_report(y_actual, y_predicted):\n",
    "    print(\"Accuracy = \" , accuracy_score(y_actual, y_predicted))\n",
    "    print(\"Precision = \" ,precision_score(y_actual, y_predicted))\n",
    "    print(\"Recall = \" ,recall_score(y_actual, y_predicted))\n",
    "    print(\"F1 Score = \" ,f1_score(y_actual, y_predicted))\n",
    "    print(\"Cohen Kappa score= \",cohen_kappa_score(y_actual,y_predicted))\n",
    "    print(\"Hamming Loss = \",hamming_loss(y_actual,y_predicted))\n",
    "    print(\"Jaccard Score = \",jaccard_score(y_actual, y_predicted))\n",
    "#     print(\"Log Loss = \",log_loss(y_actual, y_predicted))\n",
    "    print(\"Matthews correlation coefficient = \", matthews_corrcoef(y_actual, y_predicted))\n",
    "    print(\"Zero one loss = \",zero_one_loss(y_actual, y_predicted))\n",
    "#     print(\"brier_score_loss  = \",brier_score_loss(y_actual, y_predicted))\n",
    "    print(\"Balanced accuracy = \",balanced_accuracy_score(y_actual, y_predicted))\n",
    "    \n",
    "    \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Generate auc_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_auc_roc_curve(clf, X_test):\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test,  y_pred_proba)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    plt.plot(fpr,tpr,label=\"AUC ROC Curve with Area Under the curve =\"+str(auc))\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # USING STATSMODELS TO CALCULATE THE SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(y_train,X_train)\n",
    "result=logit_model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  > # LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0,class_weight = \"balanced\")\n",
    "classifier.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid={\"C\":np.logspace(-3,3,5,7), \"penalty\":[\"l1\",\"l2\"],\"max_iter\":[100,200,300,400,500]}# l1 lasso l2 ridge\n",
    "grid_cv=GridSearchCV(classifier,grid,cv=10,verbose=0)\n",
    "grid_cv.fit(X_train,y_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",grid_cv.best_params_)\n",
    "print(\"accuracy :\",grid_cv.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0,class_weight = \"balanced\",C=31.622776601683793,max_iter=200,penalty='l2')\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv =10,verbose=0)\n",
    "accuracies.std()\n",
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Changing the class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = list(y.unique())\n",
    "unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {}\n",
    "for classes in unique_classes:\n",
    "    out_dict[classes] = len(y_train)/(len(unique_classes)*y_train.value_counts(classes))\n",
    "out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(pd.Series(y_pred,name='Predicted'),\n",
    "           pd.Series(y_test,name=\"Actual\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_auc_roc_curve(classifier,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"AUC ROC Curve with Area Under the curve =\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Finding the optimal threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_idx = np.argmax(tpr-fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr-fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(tpr-fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# argmax of tpr-fpr is the optimal cut off for the decision boundary threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds[204]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the optimal threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = classifier.predict_proba(X_test)\n",
    "pd.DataFrame(classifier.predict_proba(X_test), columns=classifier.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a y_pp for assigning labels according to the optimal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pp = []\n",
    "for i in range(len(y_prob)):\n",
    "    if y_prob[i][0]>optimal_threshold:\n",
    "        y_pp.append(0)\n",
    "    else:\n",
    "        y_pp.append(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the new threshold, no need to calc auc roc again as it depends on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_test,y_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing predicted values which we got from 0.3 change in the threshold value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_pred,y_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.linspace(0.05, 0.95, 20)\n",
    "gsc = GridSearchCV(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid={\"C\":np.logspace(-3,3,5,7), \"penalty\":[\"l1\",\"l2\"],\"max_iter\":[100,200,300,400,500],\n",
    "        'class_weight': [{0: x, 1: 1.0-x} for x in weights]\n",
    "    },refit=True,\n",
    "    \n",
    "    cv=10\n",
    ")\n",
    "\n",
    "gsc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsc.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C= 1000.0,\n",
    " class_weight= {0: 0.23947368421052628, 1: 0.7605263157894737},\n",
    " max_iter= 500,\n",
    " penalty= 'l2')\n",
    "clf.fit(X_train,y_train)\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob2 = clf.predict_proba(X_test)\n",
    "y_prob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test,  y_prob2[:,1])\n",
    "auc = roc_auc_score(y_test, y_prob2[:,1])\n",
    "plt.plot(fpr,tpr,label=\"AUC ROC Curve with Area Under the curve =\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = np.argmax(tpr-fpr)\n",
    "opt_th = thresholds[opt]\n",
    "opt_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pp2 = []\n",
    "for i in range(len(y_prob2)):\n",
    "    if y_prob2[i][0]>opt_th:\n",
    "        y_pp2.append(0)\n",
    "    else:\n",
    "        y_pp2.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previous model\n",
    "generate_model_report(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previous Model\n",
    "generate_model_report(y_test,y_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_test,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_test,y_pp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = clf, X = X_train, y = y_train, cv =10,verbose=0)\n",
    "accuracies.std()\n",
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(0.8932467924807487,0.8935508466042323)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC CURVE ON THE TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = clf.predict_proba(X_train)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_train,  y_pred_proba)\n",
    "auc = roc_auc_score(y_train, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"AUC ROC Curve with Area Under the curve =\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating threshold based on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds[np.argmax(tpr-fpr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_auc_roc_curve(clf,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model without fitting the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = LogisticRegression(C= 1000,\n",
    " class_weight= {0: 0.23947368421052628, 1: 0.7605263157894737},\n",
    " max_iter= 500,\n",
    " penalty= 'l2',fit_intercept = False)\n",
    "clf2.fit(X_train,y_train)\n",
    "y_pred3 = clf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_test,y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = clf2, X = X_train, y = y_train, cv =10,verbose=0)\n",
    "accuracies.std()\n",
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The evaluation metrics doesn't change much when the intercept is not fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_auc_roc_curve(clf2,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import LearningCurve\n",
    "visualizer = LearningCurve(\n",
    "    clf, cv=10, scoring='f1',verbose = 0\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train, y_train)        # Fit the data to the visualizer\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRECISION-RECALL CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_precision_recall_curve(clf, X_test, y_test)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test,y_prob2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = list(clf2.coef_)\n",
    "m = max(coef)\n",
    "coef.index(m)\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['TrafficType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Region'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COHEN KAPPA SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "cohen_kappa_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "C=['class 0', 'class 1']\n",
    "print(classification_report(y_test, y_pred2, target_names=C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(clf, X_test, y_test)  # doctest: +SKIP\n",
    "plt.show()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAMMING LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "hamming_loss(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average precision, recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "precision, recall, threshold = precision_recall_curve(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision\n",
    "recall\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f1 and fbeta scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.precision_score(y_test, y_pred2)\n",
    "metrics.recall_score(y_test, y_pred2)\n",
    "metrics.f1_score(y_test, y_pred2)\n",
    "metrics.fbeta_score(y_test, y_pred2, beta=0.5)\n",
    "metrics.fbeta_score(y_test, y_pred2, beta=1)\n",
    "metrics.fbeta_score(y_test, y_pred2, beta=2)\n",
    "metrics.precision_recall_fscore_support(y_test, y_pred2, beta=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JACCARD SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "jaccard_score(y_test,y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HINGE LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hinge_loss\n",
    "hinge_loss(y_test,y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOG LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, y_prob2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matthews correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "matthews_corrcoef(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZERO ONE LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "zero_one_loss(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRIER SCORE LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "brier_score_loss(y_test, y_prob2[:,1])\n",
    "# brier_score_loss(y_test, 1-y_prob2[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "balanced_accuracy_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.decision_function(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of each class (prior).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.class_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Absolute additive value to variances-epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.epsilon_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theta: mean of each feature per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.theta_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nb = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nb_prob = nb.predict_proba(X_test)\n",
    "y_nb_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "C=['class 0', 'class 1']\n",
    "print(classification_report(y_test, y_nb, target_names=C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = nb, X = X_train, y = y_train, cv =10,verbose=0)\n",
    "accuracies.std()\n",
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import LearningCurve\n",
    "visualizer = LearningCurve(\n",
    "    nb, cv=10, scoring='f1',verbose = 0\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train, y_train)        # Fit the data to the visualizer\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(nb, X_test, y_test)  # doctest: +SKIP\n",
    "plt.show()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRECISION-RECALL CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_precision_recall_curve(nb, X_test, y_test)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "precision, recall, threshold = precision_recall_curve(y_test, y_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHER EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.precision_score(y_test, y_nb)\n",
    "metrics.recall_score(y_test, y_nb)\n",
    "metrics.f1_score(y_test, y_nb)\n",
    "metrics.fbeta_score(y_test, y_nb, beta=0.5)\n",
    "metrics.fbeta_score(y_test, y_nb, beta=1)\n",
    "metrics.fbeta_score(y_test, y_nb, beta=2)\n",
    "metrics.precision_recall_fscore_support(y_test, y_nb, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "hamming_loss(y_test, y_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "jaccard_score(y_test,y_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, y_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "matthews_corrcoef(y_test, y_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "zero_one_loss(y_test, y_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "brier_score_loss(y_test, y_nb_prob[:,1])\n",
    "brier_score_loss(y_test, 1-y_nb_prob[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "balanced_accuracy_score(y_test, y_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "cohen_kappa_score(y_test, y_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# nb2 = MultinomialNB()\n",
    "# nb2.fit(X_train,y_train)\n",
    "# y_nb2 = nb2.predict(X_test)\n",
    "#IT WON'T WORK AS THE VALUES HAVE NEGATIVE VALUES TOO.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with permutations for the significance of a classification score\n",
    " In order to test if a classification score is significative a technique in repeating the classification procedure after randomizing, permuting, the labels. The p-value is then given by the percentage of runs for which the score obtained is greater than the classification score obtained in the first place.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import permutation_test_score\n",
    "score, permutation_scores, pvalue = permutation_test_score(\n",
    "    GaussianNB(), X_train, y_train, scoring=\"accuracy\", cv=cv, n_permutations=100, n_jobs=1)\n",
    "print(\"Classification score %s (pvalue : %s)\" % (score, pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View histogram of permutation scores\n",
    "plt.hist(permutation_scores, 20, label='Permutation scores',\n",
    "         edgecolor='black')\n",
    "ylim = plt.ylim()\n",
    "# BUG: vlines(..., linestyle='--') fails on older versions of matplotlib\n",
    "# plt.vlines(score, ylim[0], ylim[1], linestyle='--',\n",
    "#          color='g', linewidth=3, label='Classification Score'\n",
    "#          ' (pvalue %s)' % pvalue)\n",
    "# plt.vlines(1.0 / n_classes, ylim[0], ylim[1], linestyle='--',\n",
    "#          color='k', linewidth=3, label='Luck')\n",
    "plt.plot(2 * [score], ylim, '--g', linewidth=3,\n",
    "         label='Classification Score'\n",
    "         ' (pvalue %s)' % pvalue)\n",
    "plt.plot(2 * [1. / 2], ylim, '--k', linewidth=3, label='Luck')\n",
    "\n",
    "plt.ylim(ylim)\n",
    "plt.legend()\n",
    "plt.xlabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For comparision with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import permutation_test_score\n",
    "score, permutation_scores, pvalue = permutation_test_score(\n",
    "    clf, X, y, scoring=\"accuracy\", cv=20, n_permutations=100, n_jobs=1)\n",
    "print(\"Classification score %s (pvalue : %s)\" % (score, pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View histogram of permutation scores\n",
    "plt.hist(permutation_scores, 30, label='Permutation scores',\n",
    "         edgecolor='black')\n",
    "ylim = plt.ylim()\n",
    "# BUG: vlines(..., linestyle='--') fails on older versions of matplotlib\n",
    "# plt.vlines(score, ylim[0], ylim[1], linestyle='--',\n",
    "#          color='g', linewidth=3, label='Classification Score'\n",
    "#          ' (pvalue %s)' % pvalue)\n",
    "# plt.vlines(1.0 / n_classes, ylim[0], ylim[1], linestyle='--',\n",
    "#          color='k', linewidth=3, label='Luck')\n",
    "plt.plot(2 * [score], ylim, '--g', linewidth=3,\n",
    "         label='Classification Score'\n",
    "         ' (pvalue %s)' % pvalue)\n",
    "plt.plot(2 * [1. / 2], ylim, '--k', linewidth=3, label='Luck')\n",
    "\n",
    "plt.ylim(ylim)\n",
    "plt.legend()\n",
    "plt.xlabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3,weights = 'distance')\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_knn = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_knn_prob = knn.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(knn, X_test, y_test)  # doctest: +SKIP\n",
    "plt.show()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "C=['class 0', 'class 1']\n",
    "print(classification_report(y_test, y_knn, target_names=C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = knn, X = X_train, y = y_train, cv =10,verbose=0)\n",
    "accuracies.std()\n",
    "accuracies.mean()\n",
    "#This can be misleading as it didn't change much when we changed the n_neighbours parameter from 2 to 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "cohen_kappa_score(y_test, y_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_score(y_test,y_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamming_loss(y_test,y_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_test,y_knn_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matthews_corrcoef(y_test, y_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_one_loss(y_test, y_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brier_score_loss(y_test,y_knn_prob[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test,y_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_test,y_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# permutation_test_score for knn : score and p value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import permutation_test_score\n",
    "score, permutation_scores, pvalue = permutation_test_score(\n",
    "    knn, X, y, scoring=\"accuracy\", cv=20, n_permutations=100, n_jobs=1)\n",
    "print(\"Classification score %s (pvalue : %s)\" % (score, pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View histogram of permutation scores\n",
    "plt.hist(permutation_scores, 30, label='Permutation scores',\n",
    "         edgecolor='black')\n",
    "ylim = plt.ylim()\n",
    "# BUG: vlines(..., linestyle='--') fails on older versions of matplotlib\n",
    "# plt.vlines(score, ylim[0], ylim[1], linestyle='--',\n",
    "#          color='g', linewidth=3, label='Classification Score'\n",
    "#          ' (pvalue %s)' % pvalue)\n",
    "# plt.vlines(1.0 / n_classes, ylim[0], ylim[1], linestyle='--',\n",
    "#          color='k', linewidth=3, label='Luck')\n",
    "plt.plot(2 * [score], ylim, '--g', linewidth=3,\n",
    "         label='Classification Score'\n",
    "         ' (pvalue %s)' % pvalue)\n",
    "plt.plot(2 * [1. / 2], ylim, '--k', linewidth=3, label='Luck')\n",
    "\n",
    "plt.ylim(ylim)\n",
    "plt.legend()\n",
    "plt.xlabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  DIFFERENT SCORING METRICS for GRIDSEARCHCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH FOR DIFFERENT SCORING METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={'weights':['distance','uniform'], 'n_neighbors':[3,5,7,9,12,15,20,25]}\n",
    "kn = GridSearchCV(knn, parameters,scoring='recall', cv=5)\n",
    "kn.fit(X_train,y_train)\n",
    "kn.best_params_\n",
    "params = kn.best_params_\n",
    "kn_grid = KNeighborsClassifier()\n",
    "kn_grid.set_params(**params)\n",
    "kn_grid.fit(X_train,y_train)\n",
    "y_grid = kn_grid.predict(X_test)\n",
    "generate_model_report(y_test,y_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={'weights':['distance','uniform'], 'n_neighbors':[3,5,7,9,12,15,20,25]}\n",
    "kn = GridSearchCV(knn, parameters,scoring='f1', cv=5)\n",
    "kn.fit(X_train,y_train)\n",
    "params = kn.best_params_\n",
    "kn_grid = KNeighborsClassifier()\n",
    "kn_grid.set_params(**params)\n",
    "kn_grid.fit(X_train,y_train)\n",
    "\n",
    "y_grid = kn_grid.predict(X_test)\n",
    "generate_model_report(y_test,y_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={'weights':['distance','uniform'], 'n_neighbors':[3,5,7,9,12,15,20,25]}\n",
    "kn = GridSearchCV(knn, parameters,scoring='precision', cv=5)\n",
    "kn.fit(X_train,y_train)\n",
    "kn.best_score_\n",
    "params = kn.best_params_\n",
    "kn_grid = KNeighborsClassifier()\n",
    "kn_grid.set_params(**params)\n",
    "kn_grid.fit(X_train,y_train)\n",
    "\n",
    "y_grid = kn_grid.predict(X_test)\n",
    "generate_model_report(y_test,y_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={'weights':['distance','uniform'], 'n_neighbors':[3,5,7,9,12,15,20,25]}\n",
    "kn = GridSearchCV(knn, parameters,scoring= 'balanced_accuracy',\n",
    " cv=5)\n",
    "kn.fit(X_train,y_train)\n",
    "kn.best_params_\n",
    "kn.best_score_\n",
    "params = kn.best_params_\n",
    "kn_grid = KNeighborsClassifier()\n",
    "kn_grid.set_params(**params)\n",
    "kn_grid.fit(X_train,y_train)\n",
    "\n",
    "y_grid = kn_grid.predict(X_test)\n",
    "generate_model_report(y_test,y_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={'weights':['distance','uniform'], 'n_neighbors':[3,5,7,9,12,15,20,25]}\n",
    "kn = GridSearchCV(knn, parameters,scoring= 'jaccard',\n",
    " cv=5)\n",
    "kn.fit(X_train,y_train)\n",
    "kn.best_params_\n",
    "kn.best_score_\n",
    "params = kn.best_params_\n",
    "kn_grid = KNeighborsClassifier()\n",
    "kn_grid.set_params(**params)\n",
    "kn_grid.fit(X_train,y_train)\n",
    "\n",
    "y_grid = kn_grid.predict(X_test)\n",
    "generate_model_report(y_test,y_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={'weights':['distance','uniform'], 'n_neighbors':[3,5,7,9,12,15,20,25]}\n",
    "kn = GridSearchCV(knn, parameters,scoring= 'accuracy',\n",
    " cv=5)\n",
    "kn.fit(X_train,y_train)\n",
    "kn.best_score_\n",
    "params = kn.best_params_\n",
    "kn_grid = KNeighborsClassifier()\n",
    "kn_grid.set_params(**params)\n",
    "kn_grid.fit(X_train,y_train)\n",
    "\n",
    "y_grid = kn_grid.predict(X_test)\n",
    "generate_model_report(y_test,y_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={'weights':['distance','uniform'], 'n_neighbors':[3,5,7,9,12,15,20,25]}\n",
    "kn = GridSearchCV(knn, parameters,scoring= 'roc_auc',\n",
    " cv=5)\n",
    "kn.fit(X_train,y_train)\n",
    "\n",
    "params = kn.best_params_\n",
    "kn_grid = KNeighborsClassifier()\n",
    "kn_grid.set_params(**params)\n",
    "kn_grid.fit(X_train,y_train)\n",
    "\n",
    "y_grid = kn_grid.predict(X_test)\n",
    "generate_model_report(y_test,y_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve with the best parameters given by gridsearch especially for roc_auc scoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_auc_roc_curve(kn_grid,X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM SUPPORT VECTOR MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm = svm.SVC(class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_svm = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(svm, X_test, y_test)  # doctest: +SKIP\n",
    "plt.show()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target=['class 0', 'class 1']\n",
    "print(classification_report(y_test, y_svm, target_names=target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={\"C\":[1,10,100,500,1000],'kernel':[\"linear\", \"poly\", \"rbf\", \"sigmoid\"],'gamma':['auto','scale']}\n",
    "sv = GridSearchCV(svm, parameters,scoring= 'f1',\n",
    " cv=5)\n",
    "sv.fit(X_train,y_train)\n",
    "sv.best_params_\n",
    "sv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_test,y_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT VECTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_indices = svm.support_\n",
    "len(support_vector_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of support vectors per class\n",
    "support_vectors_per_class = svm.n_support_\n",
    "print(support_vectors_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get support vectors themselves\n",
    "support_vectors = svm.support_vectors_\n",
    "support_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # DECISION TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree1 = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tree = tree1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OVERFITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_test,y_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(tree1, X_test, y_test)  # doctest: +SKIP\n",
    "plt.show()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_auc_roc_curve(tree1,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_precision_recall_curve(tree1, X_test, y_test)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># Evaluating after specifying the class weight as balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2 = DecisionTreeClassifier(class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2.fit(X_train,y_train)\n",
    "y_tree2 = tree2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_test,y_tree2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(tree2, X_test, y_test)  # doctest: +SKIP\n",
    "plt.show()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_precision_recall_curve(tree2, X_test, y_test)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_auc_roc_curve(tree2,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # FUNCTION FOR TREE REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_report(clf):\n",
    "    print(\"Depth is \", clf.get_depth())\n",
    "    print(\"n_Leaves are:  \",clf.get_n_leaves())\n",
    "#     print(\"The parameters are:\" ,clf.get_params())\n",
    "    print(\"Classes are:\" , clf.classes_)\n",
    "    print(\"The feature importances are: \" , clf.feature_importances_)\n",
    "    print(\"The inferred value of max_features:\" , clf.max_features_)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2.get_n_leaves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OVERFITTING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2.score(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # GRID SEARCH CV WITH DIFFERENT SCORING METRICS FOR COMPARING TREE SCORE ON TEST DATA FOR LOW VARIANCE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Starting with ccp_alpha = 0 (No cost complexity pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={\"max_depth\":[15,20,30,35],'min_samples_split':[5,10,15,20]}\n",
    "grid = GridSearchCV(tree1, parameters,scoring= 'accuracy',\n",
    " cv=10)\n",
    "grid.fit(X_train,y_train)\n",
    "params = grid.best_params_\n",
    "tree_grid = DecisionTreeClassifier()\n",
    "tree_grid.set_params(**params)\n",
    "tree_grid.fit(X_train,y_train)\n",
    "y_tree_grid = tree_grid.predict(X_test)\n",
    "tree_grid.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(tree_grid, feature_names=X_train, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # #The alpha value controls overfitting as it is a pruning parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #ccp_alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={\"max_depth\":[15,20,30,35],'min_samples_split':[5,10,15,20],'ccp_alpha':[0.0001]}\n",
    "grid = GridSearchCV(tree1, parameters,scoring= 'recall',\n",
    " cv=10)\n",
    "grid.fit(X_train,y_train)\n",
    "params = grid.best_params_\n",
    "tree_grid = DecisionTreeClassifier()\n",
    "tree_grid.set_params(**params)\n",
    "tree_grid.fit(X_train,y_train)\n",
    "y_tree_grid = tree_grid.predict(X_test)\n",
    "tree_grid.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_report(tree_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* >  The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.\n",
    "> \n",
    "* >  Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See sklearn.inspection.permutation_importance as an alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation importance for feature evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The permutation importance of a feature is calculated as follows. \n",
    "* First, a baseline metric, defined by scoring, is evaluated on a (potentially different) dataset defined by the X.\n",
    "* Next, a feature column from the validation set is permuted and the metric is evaluated again.\n",
    "* The permutation importance is defined to be the difference between the baseline metric and metric from permutating the feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(tree_grid, X_train, y_train, n_repeats=10,\n",
    "                                 random_state=0)\n",
    "result.importances_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # #ccp_alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={\"max_depth\":[15,20,30,35],'min_samples_split':[5,10,15,20],'ccp_alpha':[0.01]}\n",
    "grid = GridSearchCV(tree1, parameters,scoring= 'balanced_accuracy',\n",
    " cv=10)\n",
    "grid.fit(X_train,y_train)\n",
    "params = grid.best_params_\n",
    "tree_grid = DecisionTreeClassifier()\n",
    "tree_grid.set_params(**params)\n",
    "tree_grid.fit(X_train,y_train)\n",
    "y_tree_grid = tree_grid.predict(X_test)\n",
    "tree_grid.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # #alpha = 0.01 and the depth is only 2! And the model is only taking one feature for training (obviously!), this shows the extent of pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_report(tree_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTTING TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(tree_grid, feature_names=X_train, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(tree_grid, X_train, y_train, n_repeats=10,\n",
    "                                 random_state=0)\n",
    "result.importances_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # ccp_alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={\"max_depth\":[15,20,30,35],'min_samples_split':[5,10,15,20],'ccp_alpha':[0.1]}\n",
    "grid = GridSearchCV(tree1, parameters,scoring= 'f1',\n",
    " cv=10)\n",
    "grid.fit(X_train,y_train)\n",
    "params = grid.best_params_\n",
    "tree_grid = DecisionTreeClassifier()\n",
    "tree_grid.set_params(**params)\n",
    "tree_grid.fit(X_train,y_train)\n",
    "y_tree_grid = tree_grid.predict(X_test)\n",
    "tree_grid.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * ACCURACY ON TRAINING DATA IS OBVIOUSLY THE LEAST THIS TIME BECAUSE IT DIDN'T EVEN FIT.\n",
    "> * STILL THE TEST ACCURACY ID HIGH BUT IS NOT SIGNIFICANT AT ALL!!\n",
    "> * LOOK OUT FOR THE TRAP, THE EVALUATION METRICS DO NOT GIVE YOU THE SIGNIFICANCE OF THEIR SCORES.\n",
    "> * CHECK AND ANALYSE EVERY PARAMETER FOR FINAL EVALUATION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_report(tree_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * IT IS NOT TAKING ANY FEATURE INTO ACCOUNT, BECAUSE THE ENTIRE TREE GOT PRUNED!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(tree_grid, X_train, y_train, n_repeats=10,\n",
    "                                 random_state=0)\n",
    "result.importances_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * # alpha = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={\"max_depth\":[15,20,30,35],'min_samples_split':[5,10,15,20],'ccp_alpha':[0.00001]}\n",
    "grid = GridSearchCV(tree1, parameters,scoring= 'recall',\n",
    " cv=10)\n",
    "grid.fit(X_train,y_train)\n",
    "params = grid.best_params_\n",
    "tree_grid = DecisionTreeClassifier()\n",
    "tree_grid.set_params(**params)\n",
    "tree_grid.fit(X_train,y_train)\n",
    "y_tree_grid = tree_grid.predict(X_test)\n",
    "tree_grid.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_report(tree_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(tree_grid, X_train, y_train, n_repeats=10,\n",
    "                                 random_state=0)\n",
    "result.importances_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " >#  choose alpha = 10^-3 or 10^-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # min_impurity_split float, default=0\n",
    "* >  Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={\"max_depth\":[15,20,30,35],'min_samples_split':[5,10,15,20],'ccp_alpha':[0.00001]}\n",
    "grid = GridSearchCV(tree1, parameters,scoring= 'f1',\n",
    " cv=10)\n",
    "grid.fit(X_train,y_train)\n",
    "params = grid.best_params_\n",
    "tree_grid = DecisionTreeClassifier()\n",
    "tree_grid.set_params(**params)\n",
    "tree_grid.fit(X_train,y_train)\n",
    "y_tree_grid = tree_grid.predict(X_test)\n",
    "tree_grid.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_report(tree_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "text_representation = tree.export_text(tree_grid)\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #  SET MAX_DEPTH: The maximum depth of the representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(tree_grid, feature_names=X_train, filled=True,max_depth = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* > label: {all, root, none}, optional (default=all)\n",
    "* > Whether to show informative labels for impurity, etc. Options include all to show at every node, root to show only at the top root node, or none to not show at any node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "t = tree.plot_tree(tree_grid, feature_names=X_train, filled=True,max_depth = 10,label ='all',fontsize = 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Permutation test score \n",
    "> * Calculates the p-value \n",
    "> * Calculates the classification score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import permutation_test_score\n",
    "score, permutation_scores, pvalue = permutation_test_score(\n",
    "    tree_grid, X_train, y_train, scoring=\"accuracy\", cv=cv, n_permutations=100, n_jobs=1)\n",
    "print(\"Classification score %s (pvalue : %s)\" % (score, pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #  LEARNING CURVE\n",
    "> * With different scoring metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import LearningCurve\n",
    "visualizer = LearningCurve(\n",
    "    tree_grid, cv=10, scoring='f1',verbose = 0\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train, y_train)        # Fit the data to the visualizer\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import LearningCurve\n",
    "visualizer = LearningCurve(\n",
    "    tree_grid, cv=10, scoring='precision',verbose = 0\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train, y_train)        # Fit the data to the visualizer\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import LearningCurve\n",
    "visualizer = LearningCurve(\n",
    "    tree_grid, cv=10, scoring='recall',verbose = 0\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train, y_train)        # Fit the data to the visualizer\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import LearningCurve\n",
    "visualizer = LearningCurve(\n",
    "    tree_grid, cv=10, scoring='accuracy',verbose = 0\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train, y_train)        # Fit the data to the visualizer\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # PRECISION-RECALL CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_precision_recall_curve(tree_grid, X_test, y_test)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # ROC CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_auc_roc_curve(tree_grid,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # CONFUSION MATRIX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(tree_grid, X_test, y_test)  # doctest: +SKIP\n",
    "plt.show()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # CLASSIFICATION REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target=['class 0', 'class 1']\n",
    "print(classification_report(y_test, y_tree_grid, target_names=target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # EVALUATION METRICS\n",
    "> * SCORES\n",
    "> * LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_test,y_tree_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # BAGGING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(oob_score = True, class_weight = \"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clf = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* >  OVERFITTING!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_test,y_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_report2(clf):\n",
    "#     print(\"Depth is \", clf.get_depth())\n",
    "#     print(\"n_Leaves are:  \",clf.get_n_leaves())\n",
    "#     print(\"The parameters are:\" ,clf.get_params())\n",
    "    print(\"Classes are:\" , clf.classes_)\n",
    "    print(\"The feature importances are: \" , clf.feature_importances_)\n",
    "    print(\"oob score:\",clf.oob_score_)\n",
    "#     print(\"The inferred value of max_features:\" , clf.max_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_report2(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # OOB_SCORE\n",
    "* > Score of the training dataset obtained using an out-of-bag estimate.\n",
    "* > This attribute exists only when oob_score is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* > Decision function computed with out-of-bag estimate on the training set. \n",
    "* > If n_estimators is small it might be possible that a data point was never left out during the bootstrap. \n",
    "* > In this case, oob_decision_function_ might contain NaN. \n",
    "* > This attribute exists only when oob_score is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.decision_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # clf1 \n",
    "* > Changing ccp_aplha for pruning the tree and then we will evaluate the model. \n",
    "* > ccp_alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = RandomForestClassifier(oob_score = True, class_weight = \"balanced\",ccp_alpha = 0.0001)\n",
    "clf1.fit(X_train,y_train)\n",
    "y_clf1 = clf1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-0.9982765612327656"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # The training accuracy reduced by 0.0017, test by  0.0005 BUT oob_score increased, this means we always need to check every parameter for the ultimate significance of any model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_report2(clf1)\n",
    "#OOB_SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_test,y_clf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # warm_start : bool, default=False\n",
    "> # When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble,otherwise, just fit a whole new forest.\n",
    "> # Let's evaluate after setting it to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = RandomForestClassifier(oob_score = True, class_weight = \"balanced\",ccp_alpha = 0.0001, warm_start = True)\n",
    "clf2.fit(X_train,y_train)\n",
    "y_clf2 = clf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_report2(clf2)\n",
    "#OOB_SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_test,y_clf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* > Using clf1 (warm_start = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* > # Let's find the optimal max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={\"n_estimators\":[50,100,150,200],\"max_depth\":[15,20,30,35],'min_samples_split':[5,10,15,20],'min_samples_leaf':[5,10]}\n",
    "grid = GridSearchCV(clf1, parameters,scoring= 'balanced_accuracy', cv=10)\n",
    "grid.fit(X_train,y_train)\n",
    "params = grid.best_params_\n",
    "gs = RandomForestClassifier(oob_score = True, class_weight = \"balanced\",ccp_alpha = 0.0001)\n",
    "gs.set_params(**params)\n",
    "gs.fit(X_train,y_train)\n",
    "y_grid = gs.predict(X_test)\n",
    "gs.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_test,y_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_report2(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_auc_roc_curve(gs, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target=['class 0', 'class 1']\n",
    "print(classification_report(y_test, y_grid, target_names=target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(gs, X_train, y_train, n_repeats=10,\n",
    "                                 random_state=0)\n",
    "result.importances_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # PRECISION RECALL CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_precision_recall_curve(gs, X_test, y_test)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(gs, X_test, y_test)  # doctest: +SKIP\n",
    "plt.show()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import permutation_test_score\n",
    "score, permutation_scores, pvalue = permutation_test_score(\n",
    "    gs, X_train, y_train, scoring=\"accuracy\", cv=2, n_permutations=10, n_jobs=1)\n",
    "print(\"Classification score %s (pvalue : %s)\" % (score, pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import LearningCurve\n",
    "visualizer = LearningCurve( gs, cv=10, scoring='f1',verbose = 0 ) \n",
    "visualizer.fit(X_train, y_train)        # Fit the data to the visualizer \n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn=X_train\n",
    "# cn=y_train\n",
    "# fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\n",
    "# tree.plot_tree(gs.estimators_[0],\n",
    "#                feature_names = fn, \n",
    "#                class_names=cn,\n",
    "#                filled = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(gs.estimators_[0], feature_names=X_train, filled=True,max_depth = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(gs.estimators_[50], feature_names=X_train, filled=True,max_depth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # BOOSTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # XGBOOST FOR CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(tree_method = 'hist'  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_xgb = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_test,y_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_auc_roc_curve(xgb,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_precision_recall_curve(xgb, X_test, y_test)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(xgb, X_test, y_test)  # doctest: +SKIP\n",
    "plt.show()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import LearningCurve\n",
    "visualizer = LearningCurve(\n",
    "    xgb, cv=10, scoring='f1',verbose = 0\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train, y_train)        # Fit the data to the visualizer\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # To decide the value of gamma, we will take a look at the gain values (refer the formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_tree\n",
    "# rcParams['figure.figsize'] = 100,100\n",
    "fig = plot_tree(xgb,num_trees=1)\n",
    "plt.savefig(\"E:\\test.png\")\n",
    "plt.close()\n",
    "# gvz = xgb.to_graphviz(xgb_model, num_trees=xgb_model.best_iteration, rankdir=rankdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'learning_rate': [0.1,0.3, 0.5], #so called `eta` value\n",
    "              'max_depth': [5,10,15,20],\n",
    "              'min_child_weight': [0.25,0.5,1],              \n",
    "              'subsample': [0.8,1],\n",
    "              'n_estimators': [100,150,200,250,300],\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "cv = GridSearchCV(xgb, parameters,scoring='precision', cv=5)\n",
    "cv.fit(X_train,y_train)\n",
    "params = cv.best_params_\n",
    "xgb1 = XGBClassifier(tree_method = \"hist\")\n",
    "xgb1.set_params(**params)\n",
    "\n",
    "xgb1.fit(X_train,y_train)\n",
    "y_grid = xgb1.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model_report(y_test,y_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import LearningCurve\n",
    "visualizer = LearningCurve(\n",
    "    xgb1, cv=10, scoring='f1',verbose = 0\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train, y_train)        # Fit the data to the visualizer\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "cv = GridSearchCV(xgb, parameters,scoring='balanced_accuracy', cv=10)\n",
    "cv.fit(X_train,y_train)\n",
    "params = cv.best_params_\n",
    "xgb2 = XGBClassifier(tree_method = \"hist\")\n",
    "xgb2.set_params(**params)\n",
    "\n",
    "xgb2.fit(X_train,y_train)\n",
    "y_grid2 = xgb2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
